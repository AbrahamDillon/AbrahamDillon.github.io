# Toward Alignment
By Abe Dillon

## Abstract 
The [alignment problem](https://github.com/AbrahamDillon/AbrahamDillon.github.io/blob/main/musings/formalizing%20alignment.md) is central to some of the greatest existential risks facing civilization. I propose a path to achieve alignment based on a rigorous formalization of life as an information-theoretic phenomenon.

Alignment is often informally defined as ensuring the goals of artificial intelligence systems are congruent with so-called “human values”. However, there’s plenty of evidence that a consistent set of human values doesn’t exist. I argue that humans have evolved a flawed approximation to some ideal utility function which drives their behavior. That imperfect utility function varies from human to human and even over time within the same human. Thus, the key to achieving alignment with humanity is to derive that ideal utility function. 

I claim that the evolution of the flawed approximate utility function humans posess must have ultimately been driven by the Darwinian criteria: survival of the fittest. In other words the utility function evolved according to: that which has the best ability to propagate life. It makes sense that the ideal utility function that maximizes the Darwinian criteria is the criteria itself: maximize the propagation of life. In order to express that as a mathematical utility function, we must formalize the phenomenon of life.

## Background
### Motivation
Justify the claim: "The alignment problem is central to some of the greatest existential risks facing civilization."

The alignment problem is typically framed in the context of artificial intelligence: What if we set a machine to work brutally optimizing solutions to satisfy a goal that's poorly aligned with the good of humanity? In that context, much has already been written about the potential danger posed as machines become increasingly powerful at optimization. However, I believe we should consider a more general definition of the problem which omits the "artificial" qualifier because it distracts from the more general nature of the problem. We can see that humans can be unaligned with humans, or larger systems like economies can be unaligned with human interests, and octopuses can be unaligned with extraterrestrials.

The general alignment problem is not just about the hypothetical threat posed by artificial intelligence. It's at the heart of every ideological conflict and it's the reason Global Capitalism externalizes the cost of human suffering and ecological degradation. Whenever perverse incentives yield unwanted behavior in an intelligent system, whether it be addiction in the human brain or regulatory capture in a government, a poorly defined and/or implemented goal is likely to blame resulting in misalignment.

### Definition 
Clarify all the important terms 
### Criteria 
Provide "tests" for assessing success 

## Proposal 

## Discussion 
### Results 
How does the proposal satisfy the criteria?
### Insights 
Discuss interesting consequences of the proposal beyond the initial motivation.
### Open Questions 
What are some points deliberately left unanswered and why?

## Conclusion 
Intended for cursory digestion like most papers.
