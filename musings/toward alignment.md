# Toward Alignment
By Abe Dillon

# Abstract 
Solving the so-called "outer alignment problem" (refered to as just "alignment") is a critical step in mitigating the greatest existential risks currently facing human civilization including environmental degredation, war, and artificial intelligence. I propose a path to achieve alignment with living systems based on a rigorous formalization of life as an information-theoretic phenomenon.

I argue that humans have evolved a flawed approximation to some ideal utility function which drives their behavior. That imperfect utility function varies from human to human and even over time within the same human causing conflict. Thus, the key to achieving alignment with humanity is to derive that ideal utility function[^1]. 

I claim that the evolution of such a utility function must have ultimately been driven by the Darwinian criteria, "survival of the fittest". In other words, that which has the best ability to propogate life. It makes sense, then, that the ideal utility function to give an agent to maximize the darwinian criteria is the criteria itself: maximize the propogation of life. Hence, achieving alignment entails formalizing the phenomenon known as life. 




[^1]: Implementing the solution by literally changing peoples' minds seems fraught with moral concerns. If some fascists decide to screw with peoples' brains based on my writings, I, uh... I'm sorry.

# Background
## Motivation
The alignment problem is typically framed in the context of artificial intelligence: What if we set a machine to work brutally optimizing solutions to satisfy a goal that's poorly aligned with the good of humanity? In that context, much has already been written about the potential danger posed as machines become increasingly powerfull at optimization. However, I believe we should consider a more general definition of the problem which omits the "artificial" qualifier because it distracts from the more general nature of the problem. We can see that humans can be unaligned with humans, or larger systems like economies can be unaligned with human interests, and octopuses can be unaligned with extra terrestrials.

The general alignment problem is not just about the hypothetical threat posed by artificial intelligence. It's at the heart of every ideological conflict and it's the reason Global Capitalism externalizes the cost of human suffering and ecological degredation. Whenever perverse incentives yield unwanted behavior in an intelligent system, whether it be addiction in the human brain or regulatory capture in a government, a poorly defined and/or implemented goal is likely to blame resulting in misalgnment.

## Definition 
Clarify all the important terms 
## Criteria 
Provide "tests" for acessing success 

# Proposal 

# Discussion 
## Results 
How does the proposal satisfy the criteria?
## Insights 
Discuss interesting consequences of the proposal beyond the initial motivation.
## Open Questions 
What are some points deliberately left unanswered and why?

# Conclusion 
Intended for cursory digestion like most papers.
