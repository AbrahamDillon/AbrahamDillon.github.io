# Toward Alignment
By Abe Dillon

# Abstract 
The alignment problem is central to some of the greatest existential risks currently facing civilization including environmental degradation, war, and artificial intelligence incongruence. I propose a path to achieve alignment (specifically among living systems) based on a rigorous formalization of life as an information-theoretic phenomenon. 

The problem is typically stated informally in the context of ensuring the goals of artificial intelligence systems are congruent with so-called “human values”. However, there’s plenty of evidence that a consistent set of human values doesn’t exist. I argue that humans have evolved a flawed approximation to some ideal utility function which drives their behavior. That imperfect utility function varies from human to human and even over time within the same human causing conflict. Thus, the key to achieving alignment with humanity is to derive that ideal utility function[^1]. 

I claim that the evolution of such a utility function must have ultimately been driven by the Darwinian criteria: survival of the fittest. In other words the utility function evolved according to: that which has the best ability to propagate life. It makes sense that the ideal utility function that maximizes the Darwinian criteria is the criteria itself: maximize the propagation of life. In order to express that as a mathematical utility function, we must formalize the phenomenon of life.

[^1]: Implementing the solution by literally changing peoples' minds seems fraught with moral concerns. If some fascists decide to use my ideas to justify screw with peoples' brains, I apologize profusely. That's not my intent.

# Background
## Motivation
The alignment problem is typically framed in the context of artificial intelligence: What if we set a machine to work brutally optimizing solutions to satisfy a goal that's poorly aligned with the good of humanity? In that context, much has already been written about the potential danger posed as machines become increasingly powerfull at optimization. However, I believe we should consider a more general definition of the problem which omits the "artificial" qualifier because it distracts from the more general nature of the problem. We can see that humans can be unaligned with humans, or larger systems like economies can be unaligned with human interests, and octopuses can be unaligned with extra terrestrials.

The general alignment problem is not just about the hypothetical threat posed by artificial intelligence. It's at the heart of every ideological conflict and it's the reason Global Capitalism externalizes the cost of human suffering and ecological degredation. Whenever perverse incentives yield unwanted behavior in an intelligent system, whether it be addiction in the human brain or regulatory capture in a government, a poorly defined and/or implemented goal is likely to blame resulting in misalgnment.

## Definition 
Clarify all the important terms 
## Criteria 
Provide "tests" for acessing success 

# Proposal 

# Discussion 
## Results 
How does the proposal satisfy the criteria?
## Insights 
Discuss interesting consequences of the proposal beyond the initial motivation.
## Open Questions 
What are some points deliberately left unanswered and why?

# Conclusion 
Intended for cursory digestion like most papers.
