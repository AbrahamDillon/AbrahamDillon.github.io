# Toward Alignment
By Abe Dillon

## Abstract 
In this article, I propose a path to achieve alignment among intelligent systems based upon a rigorous formalization of the phenomenon of life in terms of information theory. 

I argue that the problem of aligning the goal of one agent to another extends well beyond the narrow scope of artificial agents and that misalignment among humans, human collectives (states, corporations, economic systems, etc.), and other organisms poses an existential threat to human society of roughly the same magnitude (severe) and urgency (very) regardless of any further development of artificial intelligence. I claim that misalignment among humans stems largely from variations in the objective function of each human which imperfectly approximates some ideal objective and that achieving alignment requires deriving said ideal objective. In order to derive such an objective (what a human ought to ought to do) one must find a way to circumvent Hume's law. I reason that abiogenesis must be what I call a "trans-Humean" process (pardon the tongue-in-cheek naming) since, by definition, it gives rise to goal-driven agents (organisms) where before there were none. I then propose that the goal of an organism approximates an ideal goal which can be derived from a formalization of the phenomenon of life.

For instance, one can roughly outline the objective function of a human as some reconciliation of the needs of an individual (e.g. Mazlo's hierarchy) against the demands of cooperation (e.g. the foundations of morality described in the field of moral psychology).

I introduce the alignment problem, why it's important, and what criteria will constitute a solution. Then, I explain why I believe a formalization of life is relevant, how I arrived at the above definition, and how it might lead to alignment. Finally, I evaluate my proposal against the criteria I established earlier and follow-up with discussion about what insight we gained and what questions remain open.

## Background
### Motivation
The alignment problem is typically framed in the context of artificial intelligence: What if we set a machine to work brutally optimizing solutions to satisfy a goal that's poorly aligned with the good of humanity? In that context, much has already been written about the potential danger posed as machines become increasingly powerfull at optimization. However, I believe we should consider a more general definition of the problem which omits the "artificial" qualifier because it distracts from the nature of the problem. Then we can see that humans can be unaligned with humans, or larger systems like economies can be unaligned with human interests, and octopuses can be unaligned with extra terrestrials.

The general alignment problem is not just about the hypothetical threat posed by artificial intelligence. It's at the heart of every ideological conflict and it's the reason Global Capitalism externalizes the cost of human suffering and ecological degredation. Whenever perverse incentives yield unwanted behavior in an intelligent system, whether it be addiction in the human brain or regulatory capture in a government, a poorly defined and/or implemented goal is likely to blame resulting in misalgnment.

### Definition
What the problem is. 

### Criteria 
Provide "tests" for acessing success

## Proposal


## Discussion
### Results
Evaluate the proposal against the defined criteria
### Insights
### Open Questions

## Conclusion 
